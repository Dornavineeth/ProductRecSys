{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS']=\"--master â€“driver-memory 200G pyspark-shell --executor-memory 200G\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 21:19:25 WARN Utils: Your hostname, uri-cpu003 resolves to a loopback address: 127.0.1.1; using 10.100.110.3 instead (on interface eno1)\n",
      "23/12/01 21:19:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/01 21:19:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Loading the dataset in PySpark\n",
    "# spark = SparkSession.builder.appName(\"RecommendationSystem\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"400g\") \\\n",
    "    .appName('my-cool-app') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset from CSV\n",
    "data = spark.read.csv(\"data/data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+\n",
      "| productID|rating|    reviewerID|\n",
      "+----------+------+--------------+\n",
      "|0001713353|   5.0|A1REUF3A1YCPHM|\n",
      "|0001713353|   5.0| AVP0HXC9FG790|\n",
      "|0001713353|   5.0|A324TTUBKTN73A|\n",
      "|0001713353|   5.0|A2RE7WG349NV5D|\n",
      "|0001713353|   5.0|A32B7QIUDQCD0E|\n",
      "|0001713353|   5.0|A3H9YD6K9TVKDP|\n",
      "|0001713353|   5.0| A3M314LI9OYME|\n",
      "|0001713353|   5.0|A1YDQQJDRHM0FJ|\n",
      "|0001713353|   5.0| AHXMDCU0N15TN|\n",
      "|0001713353|   5.0|A2M46WTE5TR5WN|\n",
      "|0001713353|   5.0| ATHTCOG6BB6WK|\n",
      "|0001713353|   5.0|A3MA68F62DFNVU|\n",
      "|0001713353|   3.0|A294QSAEH1Z7YI|\n",
      "|0001713353|   5.0| AC4I57VYBGO8P|\n",
      "|0001713353|   5.0|A1RR8Q97PANN98|\n",
      "|0001713353|   5.0|A1SDAYRVRR62ZH|\n",
      "|0001061240|   5.0|A1IG9N5URR82EB|\n",
      "|0001061240|   5.0| A2CVLIZ9ELU88|\n",
      "|0001061240|   5.0|A2LGACKSC0MALY|\n",
      "|0001061240|   5.0|  A6EQG0P75KHJ|\n",
      "+----------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nd = data.select(data['productID'],data['rating'],data['reviewerID'])\n",
    "nd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 21:20:05 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+---------------+----------------+\n",
      "| productID|rating|    reviewerID|productID_index|reviewerID_index|\n",
      "+----------+------+--------------+---------------+----------------+\n",
      "|0001713353|   5.0|A1REUF3A1YCPHM|        83802.0|        102888.0|\n",
      "|0001713353|   5.0| AVP0HXC9FG790|        83802.0|         66476.0|\n",
      "|0001713353|   5.0|A324TTUBKTN73A|        83802.0|        422534.0|\n",
      "|0001713353|   5.0|A2RE7WG349NV5D|        83802.0|         46199.0|\n",
      "|0001713353|   5.0|A32B7QIUDQCD0E|        83802.0|        749842.0|\n",
      "|0001713353|   5.0|A3H9YD6K9TVKDP|        83802.0|          1808.0|\n",
      "|0001713353|   5.0| A3M314LI9OYME|        83802.0|        781735.0|\n",
      "|0001713353|   5.0|A1YDQQJDRHM0FJ|        83802.0|       1191649.0|\n",
      "|0001713353|   5.0| AHXMDCU0N15TN|        83802.0|         24901.0|\n",
      "|0001713353|   5.0|A2M46WTE5TR5WN|        83802.0|        723736.0|\n",
      "|0001713353|   5.0| ATHTCOG6BB6WK|        83802.0|         11341.0|\n",
      "|0001713353|   5.0|A3MA68F62DFNVU|        83802.0|        276470.0|\n",
      "|0001713353|   3.0|A294QSAEH1Z7YI|        83802.0|        400108.0|\n",
      "|0001713353|   5.0| AC4I57VYBGO8P|        83802.0|       1072576.0|\n",
      "|0001713353|   5.0|A1RR8Q97PANN98|        83802.0|        905208.0|\n",
      "|0001713353|   5.0|A1SDAYRVRR62ZH|        83802.0|        387112.0|\n",
      "|0001061240|   5.0|A1IG9N5URR82EB|        73464.0|        886647.0|\n",
      "|0001061240|   5.0| A2CVLIZ9ELU88|        73464.0|        215268.0|\n",
      "|0001061240|   5.0|A2LGACKSC0MALY|        73464.0|         29711.0|\n",
      "|0001061240|   5.0|  A6EQG0P75KHJ|        73464.0|        194646.0|\n",
      "+----------+------+--------------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in list(set(nd.columns)-set(['rating'])) ]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(nd).transform(nd)\n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training,test)=transformed.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training.limit(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 21:20:12 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 21:20:28 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+---------------+----------------+\n",
      "| productID|rating|    reviewerID|productID_index|reviewerID_index|\n",
      "+----------+------+--------------+---------------+----------------+\n",
      "|0000013765|   5.0|A1NGDVXI2BAG7C|       157227.0|        249355.0|\n",
      "|0000013765|   5.0|A2SUAM1J3GNN3B|       157227.0|        978506.0|\n",
      "|0000230022|   5.0|A1CESEWTLAUPMN|       116646.0|        874601.0|\n",
      "|0000230022|   5.0|A22JN1BC3UQT8J|       116646.0|        692072.0|\n",
      "|0000230022|   5.0|A24EVFNLA6O0K1|       116646.0|        930177.0|\n",
      "|0000230022|   5.0|A38AAPXSJN4C5G|       116646.0|           316.0|\n",
      "|0000230022|   5.0| AFQUVVVDRNXEJ|       116646.0|         65799.0|\n",
      "|0000230022|   5.0|  AYQGXWZP7S2G|       116646.0|       1660543.0|\n",
      "|0000913154|   5.0|A28X5I7TL8BAOH|       146667.0|        523018.0|\n",
      "|0000913154|   5.0|A3S1YQA5N7DWGD|       146667.0|        193171.0|\n",
      "|0000913154|   5.0|A3S1YQA5N7DWGD|       146667.0|        193171.0|\n",
      "|0000913154|   5.0| AP25VUIFDC582|       146667.0|        288558.0|\n",
      "|000100039X|   1.0|A14A2VLURI8DGP|         1407.0|        148177.0|\n",
      "|000100039X|   1.0|A14A2VLURI8DGP|         1407.0|        148177.0|\n",
      "|000100039X|   1.0|A1FJY3P4CNXTA9|         1407.0|        150096.0|\n",
      "|000100039X|   1.0|A1XNMH3VR88PX0|         1407.0|         45498.0|\n",
      "|000100039X|   1.0|A21Y1GWYJ790D0|         1407.0|        254858.0|\n",
      "|000100039X|   1.0|A24IDHZNQNN2L2|         1407.0|        930353.0|\n",
      "|000100039X|   1.0|A27XUU2DXILHYZ|         1407.0|          1108.0|\n",
      "|000100039X|   1.0|A2B5CCPX5S77R4|         1407.0|         57801.0|\n",
      "+----------+------+--------------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/01 21:20:37 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:20:52 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:21:07 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:21:15 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:21:20 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:21:25 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:21:33 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:21:38 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:21:47 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:21:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/12/01 21:21:52 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:21:57 WARN DAGScheduler: Broadcasting large task binary with size 77.2 MiB\n",
      "23/12/01 21:22:04 WARN DAGScheduler: Broadcasting large task binary with size 77.3 MiB\n",
      "23/12/01 21:22:08 WARN DAGScheduler: Broadcasting large task binary with size 77.3 MiB\n",
      "23/12/01 21:22:15 WARN DAGScheduler: Broadcasting large task binary with size 77.3 MiB\n",
      "23/12/01 21:22:20 WARN DAGScheduler: Broadcasting large task binary with size 77.3 MiB\n",
      "ERROR:root:Exception while sending command.                       (0 + 10) / 10]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vdorna_umass_edu/.conda/envs/pysp/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vdorna_umass_edu/.conda/envs/pysp/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/vdorna_umass_edu/.conda/envs/pysp/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vdorna_umass_edu/.conda/envs/pysp/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vdorna_umass_edu/.conda/envs/pysp/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/vdorna_umass_edu/.conda/envs/pysp/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o143.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vdorna_umass_edu/ProductRecSys/train.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buri-cpu003.unity.rc.umass.edu/home/vdorna_umass_edu/ProductRecSys/train.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m als\u001b[39m=\u001b[39mALS(maxIter\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,regParam\u001b[39m=\u001b[39m\u001b[39m0.09\u001b[39m,rank\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m,userCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreviewerID_index\u001b[39m\u001b[39m\"\u001b[39m,itemCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mproductID_index\u001b[39m\u001b[39m\"\u001b[39m,ratingCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m\"\u001b[39m,coldStartStrategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdrop\u001b[39m\u001b[39m\"\u001b[39m,nonnegative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Buri-cpu003.unity.rc.umass.edu/home/vdorna_umass_edu/ProductRecSys/train.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m=\u001b[39mals\u001b[39m.\u001b[39;49mfit(training)\n",
      "File \u001b[0;32m~/.conda/envs/pysp/lib/python3.8/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pysp/lib/python3.8/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.conda/envs/pysp/lib/python3.8/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/.conda/envs/pysp/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/pysp/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.conda/envs/pysp/lib/python3.8/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o143.fit"
     ]
    }
   ],
   "source": [
    "als=ALS(maxIter=5,regParam=0.09,rank=25,userCol=\"reviewerID_index\",itemCol=\"productID_index\",ratingCol=\"rating\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "model=als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "predictions=model.transform(test)\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE=\"+str(rmse))\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs=model.recommendForAllUsers(20).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# recs=model.recommendForAllUsers(10).toPandas()\n",
    "# nrecs=recs.recommendations.apply(pd.Series) \\\n",
    "#             .merge(recs, right_index = True, left_index = True) \\\n",
    "#             .drop([\"recommendations\"], axis = 1) \\\n",
    "#             .melt(id_vars = ['reviewerID_index'], value_name = \"recommendation\") \\\n",
    "#             .drop(\"variable\", axis = 1) \\\n",
    "#             .dropna() \n",
    "# nrecs=nrecs.sort_values('reviewerID_index')\n",
    "# nrecs=pd.concat([nrecs['recommendation'].apply(pd.Series), nrecs['reviewerID_index']], axis = 1)\n",
    "# nrecs.columns = [\n",
    "        \n",
    "#         'ProductID_index',\n",
    "#         'Rating',\n",
    "#         'UserID_index'\n",
    "       \n",
    "#      ]\n",
    "# md=transformed.select(transformed['reviewerID'],transformed['reviewerID_index'],transformed['asin'],transformed['asin_index'])\n",
    "# md=md.toPandas()\n",
    "# dict1 =dict(zip(md['reviewerID_index'],md['reviewerID']))\n",
    "# dict2=dict(zip(md['asin_index'],md['asin']))\n",
    "# nrecs['reviewerID']=nrecs['UserID_index'].map(dict1)\n",
    "# nrecs['asin']=nrecs['ProductID_index'].map(dict2)\n",
    "# nrecs=nrecs.sort_values('reviewerID')\n",
    "# nrecs.reset_index(drop=True, inplace=True)\n",
    "# new=nrecs[['reviewerID','asin','Rating']]\n",
    "# new['recommendations'] = list(zip(new.asin, new.Rating))\n",
    "# res=new[['reviewerID','recommendations']]  \n",
    "# res_new=res['recommendations'].groupby([res.reviewerID]).apply(list).reset_index()\n",
    "# print(res_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
